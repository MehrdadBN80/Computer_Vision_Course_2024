{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\n\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nimport torchvision.models as models\n\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:53:46.399137Z","iopub.execute_input":"2024-06-11T22:53:46.399765Z","iopub.status.idle":"2024-06-11T22:54:04.551261Z","shell.execute_reply.started":"2024-06-11T22:53:46.399724Z","shell.execute_reply":"2024-06-11T22:54:04.550267Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-11 22:53:54.274182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-11 22:53:54.274291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-11 22:53:54.399752: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class MultiheadAttentionEinsum(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super(MultiheadAttentionEinsum, self).__init__()\n        self.num_heads = num_heads\n        self.head_dim = embedding_dim // num_heads\n\n        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        # Linear projections\n        Q = self.q_linear(query)\n        K = self.k_linear(key)\n        V = self.v_linear(value)\n\n        # Reshape and split by heads\n        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Scaled dot-product attention\n        attention_scores = torch.einsum(\"bnqd,bnkd->bnqk\", [Q, K]) / (self.head_dim ** 0.5)\n        attention_probs = F.softmax(attention_scores, dim=-1)\n\n        attended_values = torch.einsum(\"bnqk,bnvd->bnqd\", [attention_probs, V])\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n\n        # Final linear projection\n        out = self.fc_out(attended_values)\n        return out","metadata":{"id":"leTYxqjIgEuA","execution":{"iopub.status.busy":"2024-06-11T21:03:35.426630Z","iopub.execute_input":"2024-06-11T21:03:35.427446Z","iopub.status.idle":"2024-06-11T21:03:35.440950Z","shell.execute_reply.started":"2024-06-11T21:03:35.427403Z","shell.execute_reply":"2024-06-11T21:03:35.439623Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super(TransformerEncoderLayer, self).__init__()\n        self.multihead_attention = MultiheadAttentionEinsum(embedding_dim=embedding_dim, num_heads=num_heads)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embedding_dim, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, embedding_dim)\n        )\n        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n\n    def forward(self, x):\n        residual = x\n        x = self.layer_norm1(x)\n        x = x.permute(1, 0, 2)  # (seq_len, batch_size, embedding_dim)\n        attn_output = self.multihead_attention(x, x, x)[0]  # self-attention\n        x = attn_output + residual\n        x = x.permute(1, 0, 2)  # (batch_size, seq_len, embedding_dim)\n\n        residual = x\n        x = self.layer_norm2(x)\n        x = self.feed_forward(x)\n        x = x + residual\n\n        return x\n","metadata":{"id":"HuZiPWITgcl-","execution":{"iopub.status.busy":"2024-06-11T21:03:36.431776Z","iopub.execute_input":"2024-06-11T21:03:36.432466Z","iopub.status.idle":"2024-06-11T21:03:36.440468Z","shell.execute_reply.started":"2024-06-11T21:03:36.432426Z","shell.execute_reply":"2024-06-11T21:03:36.439508Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(self, num_classes, patch_size, embedding_dim, num_heads, num_layers):\n        super(VisionTransformer, self).__init__()\n        self.patch_embedding = nn.Conv2d(3, embedding_dim, kernel_size=patch_size, stride=patch_size)\n        self.positional_encoding = nn.Parameter(torch.randn(1, 14 * 14 + 1, embedding_dim))\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n        ])\n        self.fc = nn.Linear(embedding_dim, num_classes)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.patch_embedding(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = torch.cat((x, self.positional_encoding.repeat(batch_size, 1, 1)), dim=1)\n        for layer in self.transformer_layers:\n            x = layer(x)\n        x = x.mean(dim=1)\n        x = self.fc(x)\n        return x","metadata":{"id":"UXIw0dhkmYzb","execution":{"iopub.status.busy":"2024-06-11T21:03:37.433814Z","iopub.execute_input":"2024-06-11T21:03:37.434190Z","iopub.status.idle":"2024-06-11T21:03:37.443147Z","shell.execute_reply.started":"2024-06-11T21:03:37.434159Z","shell.execute_reply":"2024-06-11T21:03:37.442172Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyperparameters\nnum_epochs = 10\nbatch_size = 64\nlearning_rate = 0.001\nnum_classes = 10\npatch_size = 16\nembedding_dim = 128\nnum_heads = 8\nnum_layers = 3\n\n# CIFAR-10 dataset preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Load CIFAR-10 dataset\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# Data loaders\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hf_qf6VBgnhS","outputId":"ccb247ff-a1ec-4a2a-b1c0-7ab8f1dc35f8","execution":{"iopub.status.busy":"2024-06-11T20:40:29.432705Z","iopub.execute_input":"2024-06-11T20:40:29.433372Z","iopub.status.idle":"2024-06-11T20:40:37.933627Z","shell.execute_reply.started":"2024-06-11T20:40:29.433340Z","shell.execute_reply":"2024-06-11T20:40:37.932858Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:04<00:00, 34750619.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the model\nmodel = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1iPe3XBg8_j","outputId":"787f3263-65f7-47f0-f99b-7e36dd5784a3","execution":{"iopub.status.busy":"2024-06-11T21:03:42.884706Z","iopub.execute_input":"2024-06-11T21:03:42.885077Z","iopub.status.idle":"2024-06-11T21:03:42.914603Z","shell.execute_reply.started":"2024-06-11T21:03:42.885047Z","shell.execute_reply":"2024-06-11T21:03:42.913895Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Training loop\ntotal_steps = len(train_loader)\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n    \n    for i, (images, labels) in enumerate(train_loader_tqdm):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        train_loader_tqdm.set_postfix(loss=loss.item())\n\n    avg_loss = epoch_loss / total_steps\n    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    # Evaluate the model on the test dataset\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        accuracy = 100 * correct / total\n        tqdm.write(f\"Accuracy of the model on the test images: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:03:43.163327Z","iopub.execute_input":"2024-06-11T21:03:43.163612Z","iopub.status.idle":"2024-06-11T21:28:44.039408Z","shell.execute_reply.started":"2024-06-11T21:03:43.163587Z","shell.execute_reply":"2024-06-11T21:28:44.038404Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 782/782 [02:12<00:00,  5.91batch/s, loss=1.82]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 2.0522\nAccuracy of the model on the test images: 25.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 782/782 [02:12<00:00,  5.91batch/s, loss=2.09]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 1.9763\nAccuracy of the model on the test images: 28.05%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 782/782 [02:12<00:00,  5.89batch/s, loss=1.61]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 1.8319\nAccuracy of the model on the test images: 35.57%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 782/782 [02:12<00:00,  5.90batch/s, loss=1.6] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 1.6945\nAccuracy of the model on the test images: 38.66%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 782/782 [02:12<00:00,  5.89batch/s, loss=1.3] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 1.6117\nAccuracy of the model on the test images: 43.94%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 782/782 [02:12<00:00,  5.89batch/s, loss=1.62]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 1.5398\nAccuracy of the model on the test images: 43.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 782/782 [02:12<00:00,  5.90batch/s, loss=1.32]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 1.4701\nAccuracy of the model on the test images: 45.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 782/782 [02:12<00:00,  5.90batch/s, loss=2.05]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 1.4237\nAccuracy of the model on the test images: 48.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 782/782 [02:12<00:00,  5.88batch/s, loss=1.39]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 1.3854\nAccuracy of the model on the test images: 51.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 782/782 [02:12<00:00,  5.90batch/s, loss=1.58] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 1.3446\nAccuracy of the model on the test images: 51.09%\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(num_classes, patch_size, embedding_dim, num_heads, num_layers, num_epochs, learning_rate):\n    model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n        \n        for i, (images, labels) in enumerate(train_loader_tqdm):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            train_loader_tqdm.set_postfix(loss=loss.item())\n\n        avg_loss = epoch_loss / len(train_loader)\n        tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    # Evaluate the model on the test dataset\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        accuracy = 100 * correct / total\n        tqdm.write(f\"Accuracy of the model on the test images: {accuracy:.2f}%\")\n    \n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:38:02.512233Z","iopub.execute_input":"2024-06-10T19:38:02.513047Z","iopub.status.idle":"2024-06-10T19:38:02.523424Z","shell.execute_reply.started":"2024-06-10T19:38:02.513016Z","shell.execute_reply":"2024-06-10T19:38:02.522440Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def hyperparameter_tuning():\n    num_classes = 10\n    num_epochs = 1\n\n    # Define hyperparameters to search\n    patch_sizes = [16, 32]\n    embedding_dims = [128, 256]\n    num_heads_list = [8, 16]\n    num_layers_list = [3, 6]\n    learning_rates = [0.001, 0.0001]\n\n    best_accuracy = 0\n    best_params = {}\n\n    for patch_size in patch_sizes:\n        for embedding_dim in embedding_dims:\n            for num_heads in num_heads_list:\n                for num_layers in num_layers_list:\n                    for learning_rate in learning_rates:\n                        print(f\"Training with patch_size={patch_size}, embedding_dim={embedding_dim}, num_heads={num_heads}, num_layers={num_layers}, learning_rate={learning_rate}\")\n                        accuracy = train_model(num_classes, patch_size, embedding_dim, num_heads, num_layers, num_epochs, learning_rate)\n                        if accuracy > best_accuracy:\n                            best_accuracy = accuracy\n                            best_params = {\n                                'patch_size': patch_size,\n                                'embedding_dim': embedding_dim,\n                                'num_heads': num_heads,\n                                'num_layers': num_layers,\n                                'learning_rate': learning_rate\n                            }\n\n    print(f\"Best accuracy: {best_accuracy:.2f}%\")\n    print(\"Best hyperparameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:38:04.075363Z","iopub.execute_input":"2024-06-10T19:38:04.076022Z","iopub.status.idle":"2024-06-10T19:38:04.083662Z","shell.execute_reply.started":"2024-06-10T19:38:04.075989Z","shell.execute_reply":"2024-06-10T19:38:04.082674Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Call the hyperparameter tuning function\nhyperparameter_tuning()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T19:38:07.662139Z","iopub.execute_input":"2024-06-10T19:38:07.662977Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training with patch_size=16, embedding_dim=128, num_heads=8, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:20<00:00,  5.55batch/s, loss=2.09]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.0298\nAccuracy of the model on the test images: 27.06%\nTraining with patch_size=16, embedding_dim=128, num_heads=8, num_layers=3, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:20<00:00,  5.57batch/s, loss=1.83]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9831\nAccuracy of the model on the test images: 29.37%\nTraining with patch_size=16, embedding_dim=128, num_heads=8, num_layers=6, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:19<00:00,  3.93batch/s, loss=2.15]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.2929\nAccuracy of the model on the test images: 18.00%\nTraining with patch_size=16, embedding_dim=128, num_heads=8, num_layers=6, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:20<00:00,  3.91batch/s, loss=1.9] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9581\nAccuracy of the model on the test images: 35.47%\nTraining with patch_size=16, embedding_dim=128, num_heads=16, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:26<00:00,  5.32batch/s, loss=1.97]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9832\nAccuracy of the model on the test images: 29.44%\nTraining with patch_size=16, embedding_dim=128, num_heads=16, num_layers=3, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:26<00:00,  5.33batch/s, loss=1.81]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9752\nAccuracy of the model on the test images: 30.89%\nTraining with patch_size=16, embedding_dim=128, num_heads=16, num_layers=6, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:32<00:00,  3.68batch/s, loss=2.31]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.3218\nAccuracy of the model on the test images: 10.00%\nTraining with patch_size=16, embedding_dim=128, num_heads=16, num_layers=6, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:33<00:00,  3.67batch/s, loss=1.89]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9678\nAccuracy of the model on the test images: 31.35%\nTraining with patch_size=16, embedding_dim=256, num_heads=8, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:59<00:00,  4.37batch/s, loss=2.15]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.2053\nAccuracy of the model on the test images: 16.69%\nTraining with patch_size=16, embedding_dim=256, num_heads=8, num_layers=3, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:58<00:00,  4.37batch/s, loss=1.93]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9303\nAccuracy of the model on the test images: 34.41%\nTraining with patch_size=16, embedding_dim=256, num_heads=8, num_layers=6, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [04:39<00:00,  2.80batch/s, loss=2.33]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.3317\nAccuracy of the model on the test images: 10.00%\nTraining with patch_size=16, embedding_dim=256, num_heads=8, num_layers=6, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [04:38<00:00,  2.81batch/s, loss=1.95]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9000\nAccuracy of the model on the test images: 37.98%\nTraining with patch_size=16, embedding_dim=256, num_heads=16, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:05<00:00,  4.21batch/s, loss=2.13]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.1561\nAccuracy of the model on the test images: 17.67%\nTraining with patch_size=16, embedding_dim=256, num_heads=16, num_layers=3, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [03:06<00:00,  4.20batch/s, loss=1.87]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.9298\nAccuracy of the model on the test images: 34.66%\nTraining with patch_size=16, embedding_dim=256, num_heads=16, num_layers=6, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [04:51<00:00,  2.68batch/s, loss=2.17]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.2524\nAccuracy of the model on the test images: 15.30%\nTraining with patch_size=16, embedding_dim=256, num_heads=16, num_layers=6, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [04:51<00:00,  2.68batch/s, loss=2.01]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 1.8958\nAccuracy of the model on the test images: 35.11%\nTraining with patch_size=32, embedding_dim=128, num_heads=8, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [01:56<00:00,  6.70batch/s, loss=2.16]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.1815\nAccuracy of the model on the test images: 17.73%\nTraining with patch_size=32, embedding_dim=128, num_heads=8, num_layers=3, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [01:57<00:00,  6.68batch/s, loss=1.96]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.0865\nAccuracy of the model on the test images: 27.84%\nTraining with patch_size=32, embedding_dim=128, num_heads=8, num_layers=6, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:35<00:00,  5.04batch/s, loss=2.25]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.3211\nAccuracy of the model on the test images: 10.00%\nTraining with patch_size=32, embedding_dim=128, num_heads=8, num_layers=6, learning_rate=0.0001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 782/782 [02:35<00:00,  5.03batch/s, loss=1.98]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 2.0973\nAccuracy of the model on the test images: 27.99%\nTraining with patch_size=32, embedding_dim=128, num_heads=16, num_layers=3, learning_rate=0.001\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1:  79%|███████▉  | 618/782 [01:36<00:25,  6.35batch/s, loss=2.24]","output_type":"stream"}]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize(224),\n#     transforms.CenterCrop(224),\n    transforms.RandomRotation(20),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:04.552984Z","iopub.execute_input":"2024-06-11T22:54:04.553528Z","iopub.status.idle":"2024-06-11T22:54:04.559195Z","shell.execute_reply.started":"2024-06-11T22:54:04.553500Z","shell.execute_reply":"2024-06-11T22:54:04.558299Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:04.560202Z","iopub.execute_input":"2024-06-11T22:54:04.560484Z","iopub.status.idle":"2024-06-11T22:54:04.583320Z","shell.execute_reply.started":"2024-06-11T22:54:04.560460Z","shell.execute_reply":"2024-06-11T22:54:04.582455Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Load CIFAR-10 dataset\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=test_transform)\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:04.585492Z","iopub.execute_input":"2024-06-11T22:54:04.585731Z","iopub.status.idle":"2024-06-11T22:54:20.244251Z","shell.execute_reply.started":"2024-06-11T22:54:04.585710Z","shell.execute_reply":"2024-06-11T22:54:20.243400Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:10<00:00, 16139000.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data loaders\nbatch_size = 64\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:20.245373Z","iopub.execute_input":"2024-06-11T22:54:20.245681Z","iopub.status.idle":"2024-06-11T22:54:20.251441Z","shell.execute_reply.started":"2024-06-11T22:54:20.245657Z","shell.execute_reply":"2024-06-11T22:54:20.250375Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Best hyperparameters from tuning\nbest_params = {\n    'patch_size': 16,\n    'embedding_dim': 256,\n    'num_heads': 8,\n    'num_layers': 6,\n    'learning_rate': 0.0001\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:20.252623Z","iopub.execute_input":"2024-06-11T22:54:20.252932Z","iopub.status.idle":"2024-06-11T22:54:20.660836Z","shell.execute_reply.started":"2024-06-11T22:54:20.252892Z","shell.execute_reply":"2024-06-11T22:54:20.659709Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_model(num_classes, patch_size, embedding_dim, num_heads, num_layers, num_epochs, learning_rate):\n    model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        correct_train = 0\n        total_train = 0\n        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n        \n        for i, (images, labels) in enumerate(train_loader_tqdm):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += labels.size(0)\n            correct_train += (predicted == labels).sum().item()\n            train_accuracy = 100 * correct_train / total_train\n\n            train_loader_tqdm.set_postfix(loss=loss.item(), train_accuracy=train_accuracy)\n            train_loader_tqdm.refresh()  # Refresh tqdm display\n\n        avg_loss = epoch_loss / len(train_loader)\n        train_accuracy = 100 * correct_train / total_train\n        tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n\n        # Evaluate the model on the test dataset after each epoch\n        model.eval()\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                images = images.to(device)\n                labels = labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n            test_accuracy = 100 * correct / total\n            tqdm.write(f\"Accuracy of the model on the test images: {test_accuracy:.2f}%\")\n    \n    return test_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:29:52.125680Z","iopub.execute_input":"2024-06-11T21:29:52.126506Z","iopub.status.idle":"2024-06-11T21:29:52.139235Z","shell.execute_reply.started":"2024-06-11T21:29:52.126461Z","shell.execute_reply":"2024-06-11T21:29:52.138254Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# Train the model with the best hyperparameters for 20 epochs\nbest_accuracy = train_model(num_classes=10, \n                            patch_size=best_params['patch_size'], \n                            embedding_dim=best_params['embedding_dim'], \n                            num_heads=best_params['num_heads'], \n                            num_layers=best_params['num_layers'], \n                            num_epochs=10, \n                            learning_rate=best_params['learning_rate'])\n\nprint(f\"Best accuracy with best hyperparameters: {best_accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T21:29:58.379307Z","iopub.execute_input":"2024-06-11T21:29:58.380177Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=2, train_accuracy=28.8]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 1.9179, Train Accuracy: 28.76%\nAccuracy of the model on the test images: 36.23%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 782/782 [04:34<00:00,  2.84batch/s, loss=1.58, train_accuracy=40.3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 1.6610, Train Accuracy: 40.33%\nAccuracy of the model on the test images: 46.13%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=1.51, train_accuracy=46.2]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 1.4999, Train Accuracy: 46.19%\nAccuracy of the model on the test images: 47.15%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 782/782 [04:34<00:00,  2.85batch/s, loss=1.37, train_accuracy=50.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 1.3850, Train Accuracy: 50.59%\nAccuracy of the model on the test images: 50.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=1.37, train_accuracy=53.8] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 1.2990, Train Accuracy: 53.84%\nAccuracy of the model on the test images: 54.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=1.43, train_accuracy=56.6] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 1.2244, Train Accuracy: 56.64%\nAccuracy of the model on the test images: 58.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=1.26, train_accuracy=58.8] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 1.1742, Train Accuracy: 58.79%\nAccuracy of the model on the test images: 59.20%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=0.889, train_accuracy=60.6]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 1.1208, Train Accuracy: 60.64%\nAccuracy of the model on the test images: 60.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 782/782 [04:33<00:00,  2.86batch/s, loss=0.957, train_accuracy=62.1]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 1.0827, Train Accuracy: 62.11%\nAccuracy of the model on the test images: 61.45%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10:  57%|█████▋    | 442/782 [02:34<01:59,  2.84batch/s, loss=1.02, train_accuracy=62.8] ","output_type":"stream"}]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:27.127688Z","iopub.execute_input":"2024-06-11T22:54:27.128482Z","iopub.status.idle":"2024-06-11T22:54:27.154579Z","shell.execute_reply.started":"2024-06-11T22:54:27.128450Z","shell.execute_reply":"2024-06-11T22:54:27.153580Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define VGG model\nvgg = models.vgg16(pretrained=True)\nvgg.classifier[6] = nn.Linear(4096, 10)  # Modify the last layer to match the number of classes\nvgg = vgg.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:28:14.095364Z","iopub.execute_input":"2024-06-11T22:28:14.095722Z","iopub.status.idle":"2024-06-11T22:28:15.876824Z","shell.execute_reply.started":"2024-06-11T22:28:14.095694Z","shell.execute_reply":"2024-06-11T22:28:15.876038Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vgg.parameters(), lr=3e-4)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:28:15.878386Z","iopub.execute_input":"2024-06-11T22:28:15.878663Z","iopub.status.idle":"2024-06-11T22:28:15.883901Z","shell.execute_reply.started":"2024-06-11T22:28:15.878637Z","shell.execute_reply":"2024-06-11T22:28:15.882995Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    vgg.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n    \n    for i, (inputs, labels) in enumerate(train_loader_tqdm):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = vgg(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        train_loader_tqdm.set_postfix(loss=running_loss/(i+1), train_accuracy=(100.0*correct/total))\n        train_loader_tqdm.refresh()  # Refresh tqdm display\n    # Evaluation on test set\n    vgg.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = vgg(inputs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    print(f'Accuracy of the network on the test images: {(100.0*correct/total):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:28:15.885012Z","iopub.execute_input":"2024-06-11T22:28:15.885291Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 782/782 [07:23<00:00,  1.76batch/s, loss=0.725, train_accuracy=75.1]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 83.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 782/782 [07:24<00:00,  1.76batch/s, loss=0.387, train_accuracy=87]  \n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 85.16%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  46%|████▌     | 358/782 [03:23<03:59,  1.77batch/s, loss=0.271, train_accuracy=90.9]","output_type":"stream"}]},{"cell_type":"code","source":"# Define ResNet model\nresnet = models.resnet18(pretrained=True)\nresnet.fc = nn.Linear(resnet.fc.in_features, 10)  # Modify the last layer to match the number of classes\nresnet = resnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:29.974219Z","iopub.execute_input":"2024-06-11T22:54:29.974579Z","iopub.status.idle":"2024-06-11T22:54:30.385888Z","shell.execute_reply.started":"2024-06-11T22:54:29.974550Z","shell.execute_reply":"2024-06-11T22:54:30.385077Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet.parameters(), lr=3e-4)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:32.678687Z","iopub.execute_input":"2024-06-11T22:54:32.679334Z","iopub.status.idle":"2024-06-11T22:54:32.684576Z","shell.execute_reply.started":"2024-06-11T22:54:32.679306Z","shell.execute_reply":"2024-06-11T22:54:32.683595Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(5):\n    resnet.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{5}\", unit=\"batch\")\n    \n    for i, (inputs, labels) in enumerate(train_loader_tqdm):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = resnet(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        train_loader_tqdm.set_postfix(loss=running_loss/(i+1), train_accuracy=(100.0*correct/total))\n        train_loader_tqdm.refresh()  # Refresh tqdm display\n        \n    # Evaluation on test set\n    resnet.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = resnet(inputs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    print(f'Accuracy of the network on the test images: {(100.0*correct/total):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T22:54:46.182926Z","iopub.execute_input":"2024-06-11T22:54:46.183531Z","iopub.status.idle":"2024-06-11T23:09:20.991029Z","shell.execute_reply.started":"2024-06-11T22:54:46.183501Z","shell.execute_reply":"2024-06-11T23:09:20.990102Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 782/782 [02:36<00:00,  5.01batch/s, loss=0.354, train_accuracy=88]  \n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 90.71%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 782/782 [02:35<00:00,  5.04batch/s, loss=0.162, train_accuracy=94.6]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 90.74%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 782/782 [02:35<00:00,  5.03batch/s, loss=0.109, train_accuracy=96.2] \n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 92.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 782/782 [02:34<00:00,  5.05batch/s, loss=0.0847, train_accuracy=97.1]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 91.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 782/782 [02:35<00:00,  5.03batch/s, loss=0.0631, train_accuracy=97.9]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the network on the test images: 92.07%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define ViT model with ignore_mismatched_sizes\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nvit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=10, ignore_mismatched_sizes=True).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T23:11:09.303463Z","iopub.execute_input":"2024-06-11T23:11:09.303847Z","iopub.status.idle":"2024-06-11T23:11:11.060216Z","shell.execute_reply.started":"2024-06-11T23:11:09.303808Z","shell.execute_reply":"2024-06-11T23:11:11.059358Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(vit.parameters(), lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T23:11:19.481665Z","iopub.execute_input":"2024-06-11T23:11:19.482071Z","iopub.status.idle":"2024-06-11T23:11:19.488837Z","shell.execute_reply.started":"2024-06-11T23:11:19.482041Z","shell.execute_reply":"2024-06-11T23:11:19.487917Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(5):\n    vit.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{5}\", unit=\"batch\")\n    \n    for i, (inputs, labels) in enumerate(train_loader_tqdm):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = vit(inputs).logits\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        train_loader_tqdm.set_postfix(loss=running_loss/(i+1), train_accuracy=(100.0*correct/total))\n        train_loader_tqdm.refresh()  # Refresh tqdm display\n\n    # Evaluation on test set\n    vit.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = vit(inputs).logits\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n    print(f'Accuracy of the ViT model on the test images: {(100.0*correct/total):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T23:11:26.983361Z","iopub.execute_input":"2024-06-11T23:11:26.984088Z","iopub.status.idle":"2024-06-11T23:32:20.677416Z","shell.execute_reply.started":"2024-06-11T23:11:26.984059Z","shell.execute_reply":"2024-06-11T23:32:20.676283Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 782/782 [16:11<00:00,  1.24s/batch, loss=0.116, train_accuracy=96.6]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy of the ViT model on the test images: 96.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  21%|██▏       | 168/782 [03:30<12:48,  1.25s/batch, loss=0.0295, train_accuracy=99.1]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}