{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leTYxqjIgEuA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiheadAttentionEinsum(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(MultiheadAttentionEinsum, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        ################  Must be implemented  ################\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        #######################################################\n",
    "        # Linear projection\n",
    "        # out = self.fc_out(attended_values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuZiPWITgcl-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.multihead_attention = MultiheadAttentionEinsum(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, embedding_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, embedding_dim)\n",
    "        attn_output = self.multihead_attention(x, x, x)[0]  # self-attention\n",
    "        x = attn_output + residual\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        residual = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXIw0dhkmYzb"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, patch_size, embedding_dim, num_heads, num_layers):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding = nn.Conv2d(3, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 14 * 14 + 1, embedding_dim))\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = torch.cat((x, self.positional_encoding.repeat(batch_size, 1, 1)), dim=1)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hf_qf6VBgnhS",
    "outputId": "ccb247ff-a1ec-4a2a-b1c0-7ab8f1dc35f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:05<00:00, 29405465.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "patch_size = 16\n",
    "embedding_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "\n",
    "# CIFAR-10 dataset preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1iPe3XBg8_j",
    "outputId": "787f3263-65f7-47f0-f99b-7e36dd5784a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/782], Loss: 2.1098\n",
      "Epoch [1/10], Step [200/782], Loss: 2.1042\n",
      "Epoch [1/10], Step [300/782], Loss: 2.0569\n",
      "Epoch [1/10], Step [400/782], Loss: 2.0043\n",
      "Epoch [1/10], Step [500/782], Loss: 1.9699\n",
      "Epoch [1/10], Step [600/782], Loss: 2.0599\n",
      "Epoch [1/10], Step [700/782], Loss: 1.7518\n",
      "Epoch [2/10], Step [100/782], Loss: 2.0259\n",
      "Epoch [2/10], Step [200/782], Loss: 1.7581\n",
      "Epoch [2/10], Step [300/782], Loss: 1.9223\n",
      "Epoch [2/10], Step [400/782], Loss: 1.8391\n",
      "Epoch [2/10], Step [500/782], Loss: 1.9228\n",
      "Epoch [2/10], Step [600/782], Loss: 1.7407\n",
      "Epoch [2/10], Step [700/782], Loss: 1.8338\n",
      "Epoch [3/10], Step [100/782], Loss: 1.7690\n",
      "Epoch [3/10], Step [200/782], Loss: 1.6399\n",
      "Epoch [3/10], Step [300/782], Loss: 1.6874\n",
      "Epoch [3/10], Step [400/782], Loss: 1.8803\n",
      "Epoch [3/10], Step [500/782], Loss: 1.7312\n",
      "Epoch [3/10], Step [600/782], Loss: 1.6702\n",
      "Epoch [3/10], Step [700/782], Loss: 1.6725\n",
      "Epoch [4/10], Step [100/782], Loss: 1.4914\n",
      "Epoch [4/10], Step [200/782], Loss: 1.6974\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = VisionTransformer(num_classes, patch_size, embedding_dim, num_heads, num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    ################  Must be implemented  ################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoG3m2IPhCoH"
   },
   "outputs": [],
   "source": [
    "#Testing Phase\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    ################  Must be implemented  ################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # accuracy = 100 * correct / total\n",
    "    # print(f'Test Accuracy of the model on the {total} test images: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
